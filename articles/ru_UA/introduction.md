## Импорт данных

Наша компания разрабатывает продукт для агентств недвижимости. А любое агентство недвижимости работает с данными. Часто этих данных много, иногда очень много. Пока они хранятся в базе данных, об их объёме не часто задумываешься. Осознавать объёмы мы начинаем, когда необходимо перенести всю базу из одной системы в другую.

Когда у нас появился один крупный клиент, нам пришлось задуматься о том, как перенести записи о миллионах объектов недвижимости из множества разношёрстных систем, которыми этот клиент пользовался до нас, — в нашу CRM.

Мы написали сервисы, предоставляющие API для импорта данных. Такой эндпоинт принимает пачку в 1000 объектов недвижимости — и раскладывает их по базам данных, следит за авторизацией, квотами и рейт-лимитами.

И вот, очередь дошла до написания клиента. Инженеры по интеграции со стороны клиента, конечно, уже выкатили собственное решение на основе наших спецификаций. Но мы хотели сделать собственный клиент, который бы всегда соответствовал самой свежей версии нашей спецификации, был бы стабилен и предоставлял нам крэш-логи в правильном формате. Короче, мы сели писать консольную утилиту, которая бы принимала на вход огромные JSONLine файлы — и отправляла бы данные на наши серверы.

Итак, задача:

-   дан JSONline файл, каждая строчка которого представляет собой JSON с обьектом недвижимости. Файл может содержать миллионы строчек и весить гигабайты. В принципе, файл может быть не один, а несколько. Данные могут быть разбиты по файлам произвольно, но это ничего не меняет
-   дан GraphQL API, который принимает, условно, до 1000 объектов недвижимости за один запрос — и отвечает списком ID успешно и неуспешно импорированных объектов
-   нужно считывать данные потоком, паковать в пачки по 1000 штук и отправлять на сервер, обрабатывая ошибки и иногда ответы рейт-лимитера

## Потоки

Node.js с самых первых своих версий знакомит нас с концепцией потоков (streams). Сначала это были потоки чисто бинарных или текстовых данных, с помощью которых можно было оперировать буферами или строками. Потом в какой-то версии добавили object mode, позволяющий оперировать объектами.

Чтобы читать большие файлы не целиком в память, а по кусочку, использование потоков может быть удобно.

Начнём

```ts
import { parse as parseJsonLines } from "jsonlines";

const propertiesStream = createReadStream(fileName, {
    encoding: "utf-8",
    autoClose: true,
}).pipe(parseJsonLines());
```

И вот, у нас есть поток объектов недвижимости `propertiesStream`.

Мы будем строить цепочку операций, которая в итоге приведёт к отправке данных на наш сервер.

Следующим шагом этого конвеера мы должны группировать потоки в пачки по 1000 штук.

Но как это сделать? Тут надо упомянуть о том, что можно продолжать работать в парадигме потоков, написать какой-нибудь transform stream, в который с помощью метода pipe можно передать поток объектов недвижимости, а на выходе у него будут пачки.

Что-нибудь вот эдакое:

```ts
import { Transform } from "stream";

// ...

const chunk = (size: number) => {
    const batch: any[] = [];

    return new Transform({
        objectMode: true,
        transform: (chunk, encoding, callback) => {
            batch.push(chunk);

            if (batch.length >= size) {
                callback(null, batch);
                batch.length = 0; // Clear the batch
            } else {
                callback(); // Continue processing
            }
        },
        flush: (callback) => {
            if (batch.length > 0) {
                callback(null, batch);
            } else {
                callback();
            }
        },
    });
};

const propertiesBatchStream = propertiesStream.pipe(chunk(1000));
```

Но мне не нравилось то, что Transform практически никак не проверяет типы на этапе компиляции. Был бы это дженерик, навязывающий соответствие типов входного потока типам, передаваемым в transform функцию, и выводящий тип результата... Но стандартная библиотека так не умеет.

Поэтому прибегнем к асинхронным итераторам.

## Итераторы

Любой Node.js поток реализует интерфейс `AsyncIterable`. А это значит, мы можем его читать с помощью `for await...of`. Поэтому вместо Transform я должен использовать итератор:

```ts
import { parse as parseJsonLines } from "jsonlines";

const propertiesStream = createReadStream(fileName, {
    encoding: "utf-8",
    autoClose: true,
}).pipe(parseJsonLines());

async function* getPropertiesBatches(
    propertiesStream: AsyncIterable<Property>,
) {
    const batch: any[] = [];
    for await (const property of propertiesStream) {
        batch.push(property);
        if (batch.length >= 1000) {
            yield batch;
            batch.length = 0;
        }
    }
    if (batch.length > 0) {
        yield batch;
    }
}
```

Теперь у нас есть функция, итерируя по результатам которой, мы можем работать уже не с объектами недвижимости, а с пачками таких объектов:

```ts
// ...

for await (const batch of getPropertiesBatches(propertiesStream)) {
    // do something with batch. For, example, send it to the server
}
```

Уже хорошо! Не знаю, как вам, а для меня синтаксис асинхронных итераторов читается более лаконично и просто, чем вариант с Transform потоком.

Кроме того, у нас есть из коробки проверка типов. `getPropertiesBatches()` возвращает поток пачек объектов недвижимости. В нашем случае это `AsyncIterable<Property[]>`, и этот тип автоматически выводится TypeScript-ом.

Однако, кроме Properties у нас ещё будут множество разных сущностей, которые хотелось бы обрабатывать подобным способом. Это значит, "упаковщик в пачки" нужно сделать реюзабельной функцией. Например, можно сделать так:

```ts
async function* makeBatch<T>(
    source: AsyncIterable<T>,
    batchSize: number,
): AsyncIterable<T[]> {
    const batch: T[] = [];
    for await (const item of source) {
        batch.push(item);
        if (batch.length >= batchSize) {
            yield batch;
            batch.length = 0;
        }
    }
    if (batch.length > 0) {
        yield batch;
    }
}
```

Теперь, благодаря дженерику T, функция будет упаковывать в пачки итераторы любых сущностей. Но только асинхронные итераторы. А хотелось бы сделать её универсальной, и позволить обрабатывать синхронные итераторы. Например, массивы.

```ts
// The followong code will not compile:
const b = await makeBatch([1, 2, 3, 4, 5], 2);
console.log(b);
// [[1, 2], [3, 4], [5]]
```

Код не скомпилируется, потому что массив не реализует интерфейс `AsyncIterable`. Но мы можем сделать это, и позволить обрабатывать синхронные итераторы. Введём тип `Iter`, которому будет пофиг на то, синхронный он или асинхронный:

```ts
type Iter<T> = Iterable<T> | AsyncIterable<T>;
```

И теперь сигнатура функции `makeBatch()` будет:

```ts
async function* makeBatch<T>(
    source: Iter<T>,
    batchSize: number,
): AsyncIterable<T[]>;
```

А сама реализация функции не изменится.

Отлично. У нас есть первый хелпер - `makeBatch`. И есть обобщённый тип для любых итераторов - `Iter<T>`.

Теперь нам надо научиться отправлять данные на сервер, передавая их в SDK, и обрабатывать ответы от сервера, передавая их через асинхронный итератор дальше по цепочке конвейера.

Иными словами, мы должны преобразовать данные типа `Property[]` в `PropertyBatchAPICallResult`. Представим, что есть `PropertyBatchAPICallResult` - это наш тип данных, который содержит исчерпывающую информацию об одном вызове API: идентификаторы успешно и неудачно импортированных объектов, возможно, ответы рейт-лимитера, любые ошибки (авторизации, сети итд). В нашей цепочке мы не будем оперировать исключениями, а вместо этого мы будем любые ошибки оборачивать в тип данных, который мы будем использовать в дальнейшем.

В общем, ясно, что нам надо написать некую обёртку над API-методом, которая бы принимала `Property[]`, отправляла бы его на сервер и возвращала бы `PropertyBatchAPICallResult`. И через эту функцию мы должны пропускать каждый элемент итератора. Но это же map! У массива можно вызывать map, а у асинхронного итератора такого метода нет. Давайте напишем его.

```ts
function* map<T, U>(source: Iter<T>, mapper: (item: T) => Promise<U> | U) {
    for await (const item of source) {
        yield await mapper(item);
    }
}
```

Тепер, когда у нас есть `map`, мы можем написать так:

```ts
const propertiesStream = createReadStream(fileName, {
    encoding: "utf-8",
    autoClose: true,
}).pipe(parseJsonLines());

const batchesIter = getPropertiesBatches(propertiesStream);

const serverResponsesIter = map(
    batchesIter,
    async (batch: Property[]): Promise<PropertyBatchAPICallResult> => {
        try {
            const response = await SDK.importPropertiesBatch(batch);
            return processResponse(response);
        } catch (e) {
            return processError(e);
        }
    },
);

// And now let's write logs:

for await (const response of serverResponsesIter) {
    logger.write(response);
}
```

Для простоты я тут не показыва обработку ответов рейт-лимитера.

Вот, в принципе, и весь конвейер. Напоминаю, что данные обрабатываются по цепочке, то есть, весь json ни в коем случае не вычитывается в память целиком. Он считывается по одной строчке, строчки группируются по пачкам, пачки улетают на сервер, и пока сервер не ответит на первую пачку, 1001-ая строчка JSON-а не будет читаться. Конвейер обработки будет ждать. Этот эффект похож на обратное давление. И это прекрасно, потому что так гарантируется, что память никогда не будет расти неограниченно и непредсказуемо.

## Конвейер (chain)

Но мне не сильно нравится то, как выглядит код.

Я хотел бы, чтобы асинхронными итераторами можно было бы манипулировать так же красиво, как массивами.

```js
const result = [1, 2, 3, 4].map(/*...*/).filter(/*...*/).reduce(/*...*/);
```

Настало время переписать наши хелперы так, чтобы можно было бы написать нечто вроде:

```ts
const stream = createReadStream(fileName, {
    encoding: "utf-8",
    autoClose: true,
}).pipe(parseJsonLines());

chain(stream).batch(1000).map(importPropertiesBatch).map(writeLogs);
```

Нам надо реализовать композицию методов цепочкой.

Но для этого перепишем наш пример так:

```ts
// (stream reader is not shown here)

await chain(stream)
    .pipe(batch(1000))
    .pipe(map(importPropertiesBatch))
    .pipe(map(writeLogs))
    .consume();
```

Как видно из примера, нам нужно написать класс с двумя методами: `pipe` и `consume`.

Метод `pipe` позволяет трансформировать итератор через функцию.

Метод `consume` собственно итерирует по итератору.

```ts
class Chain<T> {
    constructor(private source: Iter<T>) {}
    pipe<O>(op: Operator<T, O>): Chain<O> {
        return new Chain<O>(op(this.source));
    }
    async consume(): Promise<void> {
        for await (const value of this.source) {
        }
    }
}
```

В нашем словаре появился новый термин: `Operator`. Это всего лишь функция, которая принимает итератор одного типа и возвращает новый итератор другого типа:

```ts
interface Operator<I, O> {
    (input: Iter<I>): Iter<O>;
}
```

И вот теперь, когда у нас есть такие примитивы, как конвейер и оператор, и когда мы можем сказать, что конвейер - это композиция последовательно применяемых функций-операторов, то мы можем пофантазировать дальше и придумать ещё кучу полезных операторов:

-   у нас уже есть:
    -   `batch` - собирает элементы итератора по пачкам
    -   `map` - применяет функцию к каждому элементу итератора
-   почему бы не добавить:
    -   `tap` - вызывает функцию для каждого элемента итератора, но не меняет итератор
    -   `concurrentMap` - применяет функцию к каждому элементу итератора с настраиваемым уровнем параллелизма
    -   `filter` - фильтрует элементы
    -   `take` - выбирает первые N элементов
    -   `skip` - пропускает первые N элементов
    -   и т.д.

Так появилась идея написания отдельной библиотеки `@sweepbright/iter-helpers`. Всё вышеперечисленное и многое другое там уже реализовано.

[Попробуйте, вам понравится!](https://github.com/sweepbright/iter-helpers)

## Fifo

Однако, в реальной жизни всё, как всегда, бывает немного сложнее.

У каждого объекта недвижимости, который мы импортируем через наше API, есть картинки - фотографии интерьера, чертежи планировки и т.д. Их тоже надо импортировать, но уже через другой API. И если объекты недвижимости импортируются пачками, то картинки - по одной. Предположим, что у нас есть пачка в 1000 объектов недвижимости. А картинок у каждой квартиры может быть около 20. Тогда нам надо сделать один запрос на импорт данных - и 20000 запросов на импорт картинок.

Мы, конечно, можем пойти в лоб, и сделать примерно так:

```ts
await chain(stream)
    .pipe(batch(1000))
    .pipe(map(importPropertiesBatch))
    .pipe(tap(writeLogs))
    // finds image files for each property and returns them one by one
    .pipe(map(getPropertiesImages)) // -> Iter<PropertyImage>
    .pipe(map(importPropertyImage))
    // etc
    .consume();
```

Но тогда будет тратиться драгоценное время на то, чтобы дождаться, когда обратное давление со стороны `.pipe(map(importPropertyImage))` и только тогда можем отправлять следующую пачку данных на сервер (`.pipe(map(importPropertiesBatch))`).

Такая жёсткая синхронизация неудобна, потому что сервис картинок может иметь свой отдельный рейт-лимитер. К тому же, его производительность в разные моменты может быть различной, и хотелось бы, чтобы два процесса - импорт данных и импорт картинок не имели взаимного влияния.

Вот бы можно было ответвиться от конвейера импорта данных - и создать отдельный конвейер импорта картинок. Чтобы можно было на вход конвейера импорта картинок направить итератор, скажем, с идентификаторами объектов недвижимости, а он бы уже в своём темпе читал и импортировал бы файлы картинок.

Но тут есть одна проблема. У итератора pull-семантика. В итератор нельзя сделать `.push()`, это вам не массив.

Чтобы не ходить вокруг да около, в библиотеке `@sweepbright/iter-helpers` я реализовал асинхронный итератор, в который можно сделать `.push()`. По сути, это FIFO очередь, по которой можно итерировать. Код приводить не буду, он достаточно запутанный, но вот как его можно использовать:

```ts
const f = new Fifo<number>();

chain(f).pipe(tap(console.log)).consume();

f.push(1);
f.push(2);
f.push(3);
// ...
f.end(); //<- iteration ends
```

"Но где же обратное давление?", спросите вы. Действительно, если бы в предыдущем примере я бесконечно пушил в `f` числа, быстрее, чем бы они обрабатывались, то и очередь в Fifo росла бы соответственно бесконечно. Чтобы предотвратить это, можно ввести ограничение на размер очереди. Но тогда перед каждым `push` мы должны сначала убедиться, что очередь не переполнена.

```ts
const f = new Fifo<number>({
    highWatermark: 100,
});

chain(f).pipe(tap(console.log)).consume();

await f.waitDrain();
f.push(1);

await f.waitDrain();
f.push(2);

await f.waitDrain();
f.push(3);
// ...
f.end(); //<- iteration ends
```

`waitDrain` возвращает промис, который ожидает пока размер очереди не снизится ниже "ватерлинии", то есть предела.

Вооружившись `Fifo`, мы можем переписать наш импортер так, чтобы у нас было два независимых конвейера - один для данных, а другой для картинок.

```ts
const dataImportPipeline = chain(stream)
    .pipe(batch(1000))
    .pipe(map(importPropertiesBatch))
    .pipe(tap(writeLogs))
    .pipe(
        tap((batchImportResult) => {
            const ids = getSuccessfullyImportedPropertyIDs(batchImportResult);
            for (const id of ids) {
                await propertyIDsFifo.waitDrain();
                propertyIDsFifo.push(data.id);
            }
        }),
    )
    .consume();

// This is our FIFO queue, which connects the two pipelines
const propertyIDsFifo = new Fifo<string>({
    highWatermark: 10000, // let's assume we can keep 10k property IDs in the memory
});

const imageImportPipeline = chain(propertyIDsFifo)
    .pipe(map(getPropertyImages))
    .pipe(map(importPropertyImage))
    .pipe(tap(writeLogs))
    .pipe(
        // onEnd operator's callback will be called once the pipeline is done
        onEnd(() => {
            // we must end the fifo queue, otherwise the iteration will hang forever
            propertyIDsFifo.end();
        }),
    );

// Starting both pipelines in parallel
await Promise.all([
    dataImportPipeline.consume(),
    imageImportPipeline.consume(),
]);

console.log("import done");
```

Здесь, обратное давление из `imageImportPipeline` в `dataImportPipeline` пойдёт только если в очереди накопится более 10000 идентификаторов объектов недвижимости. То есть, очередь позволяет связать выход одного конвейера со входом другого, но, в то же время, настраивать уровень буфферизации, что позволяет, если надо, смягчить временные зависимости между исполнением двух конвейеров.

## Пишем UI. Мультиплексирование логов и метрик

В реальном проекте у нас немного больше различных конвейеров импорта данных. И все они порождают метрики и логи, которые хотелось бы показывать на UI.

Приведу упрощённый пример, без метрик. Каждый конвейер импорта данных толкает логи в отдельную очередь. А теперь из всех трёх очередей надо читать логи и отображать их в интерфейсе:

```ts
async function displayLogs(
    contactLogs: Fifo<LogItem>,
    propertyLogs: Fifo<LogItem>,
    propertyImageLogs: Fifo<LogItem>,
) {
    await Promise.all([
        chain(contactLogs).pipe(tap(console.log)).consume(),
        chain(propertyLogs).pipe(tap(console.log)).consume(),
        chain(propertyImageLogs).pipe(tap(console.log)).consume(),
    ]);
}
```

Ну, можно как-то так. Создаём три конвейера, и каждый пишет что-то в консоль. А что если мы хотим писать не только в консоль, но и в файл? В один и тот же. Но записи могут приходить одновременно, а это уже не очень хорошо. Мы никак не сериализируем потенциально параллельные операции!

И с точки зрения красоты кода, приведённый выше подход тоже проигрывает. Что если нам пришлось бы навешивать множество одинаковых обработчиков для каждого конвейера логов?

Вот бы можно было объединить, или мультиплексировать, несколько итераторов в один!

Для этого в библиотеке есть функция `mux`.

```ts
async function displayLogs(
    contactLogs: Fifo<LogItem>,
    propertyLogs: Fifo<LogItem>,
    propertyImageLogs: Fifo<LogItem>,
) {
    await chain(mux([contactLogs, propertyLogs, propertyImageLogs]))
        .pipe(tap(console.log))
        .consume();
}
```

## Конец

В заключение хотелось бы отметить, что библиотека уже используется в двух наших проектах, в которых есть долгоживущие процессы, обрабатывающие большие массивы данных. Поэтому баги периодически фиксятся, и библиотека активно развивается.

Многие примеры, показанные в статье, могут быть записаны проще, потому что класс `Chain` имеет не только метод `pipe`, но и шорткаты для всех библиотечных операторов. Так, вместо:

```ts
chain(/*...*/).pipe(map(/*...*/));
```

можно написать:

```ts
chain(/*...*/).map(/*...*/);
```

и т.д.

### P.S.

Про ECMAScript proposal [Async Iterator Helpers](https://github.com/tc39/proposal-async-iterator-helpers) я в курсе.

Оно во многом похоже на мою библиотеку, но `@sweepbright/iter-helpers` предоставляет больше возможностей, в то же время он, скорее всего, не будет совместим с будущей спецификацией "Async Iterator Helpers".

## Ссылки

-   [@sweepbright/iter-helpers](https://github.com/sweepbright/iter-helpers)
